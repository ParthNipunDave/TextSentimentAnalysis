{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zgiV7F_zdsYv"
   },
   "outputs": [],
   "source": [
    "from flask import Flask,request,jsonify,render_template\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re,string,nltk\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from keras.models import model_from_json\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stem = PorterStemmer()\n",
    "import time,datetime\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "S4CTW4oCgqY9",
    "outputId": "f71b150c-9cbd-451e-bdba-00b686c3aa7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "json_file = open('SentimentAnalysis.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights(\"SentimentAnalysis.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sa302OzrduVV"
   },
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "# @app.route('/prediction/<text_data>')\n",
    "\n",
    "@app.route('/')\n",
    "def sentiment():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/prediction',methods=['GET'])\n",
    "def prediction():\n",
    "    if request.method == 'GET':\n",
    "        text_data = request.args.get('text_data')\n",
    "        \n",
    "    print(text_data)\n",
    "    labels= {'0':'Negative','1':'Positive'}\n",
    "#     text = request.form('text_data')\n",
    "    def remove_emoji(string):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', string)\n",
    "    def clean_data(txt):\n",
    "        txt = ' '.join([word for word in txt.split() if word not in re.findall(\"^@.+\",word)])\n",
    "        txt = ' '.join([word for word in txt.split() if word not in re.findall(\"^http.+|^www\\..+|[a-zA-Z0-9]+[.][a-zA-Z]{2,4}$\",word)])\n",
    "        txt = ' '.join([word for word in txt.split() if word not in re.findall(\"^:.+|^;.+|^XD|^xD\",word)])\n",
    "        txt = remove_emoji(txt)\n",
    "        txt = ''.join([word.lower() for word in txt if word not in string.punctuation])\n",
    "        txt = ' '.join([lemmatizer.lemmatize(word) for word in txt.split() if word not in stopwords.words('english')])\n",
    "        txt = word_tokenize(txt)\n",
    "        return txt\n",
    "    text = clean_data(text_data)\n",
    "    text_len = loaded_model.input_shape[1]\n",
    "    token_obj = Tokenizer()\n",
    "    token_obj.fit_on_texts(text)\n",
    "    sequences = token_obj.texts_to_sequences([text])\n",
    "    word_index = token_obj.word_index\n",
    "    seq_pad = pad_sequences(sequences,maxlen=text_len)\n",
    "    predict = loaded_model.predict_classes(seq_pad)\n",
    "    #out = {'Sentiment of text is ':labels[str(predict[0][0])]}\n",
    "    #     print(jsonify(labels[str(predict[0][0])]))\n",
    "#     result = labels[str(predict[0][0])]\n",
    "    return render_template(\"output.html\",result = labels[str(predict[0][0])]) \n",
    "\n",
    "  \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "wzcW3M0PhquT",
    "outputId": "5617ce08-db28-4d88-a7c8-1ec51bb5c5c3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "app.run(debug=True,use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O24NsrRxh3z-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBEK2d1xiA4E"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Deploy Sentiment Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
